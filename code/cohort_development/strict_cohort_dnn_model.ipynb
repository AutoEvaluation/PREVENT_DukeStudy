{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6552caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/strict_cohort2/all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class DiscreteTimeNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes, num_bins):\n",
    "        super(DiscreteTimeNN, self).__init__()\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            torch.nn.LazyLinear(size)\n",
    "            for size in hidden_layer_sizes\n",
    "        ])\n",
    "        \n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.prediction_head = torch.nn.LazyLinear(num_bins + 1)\n",
    "        self.softmax = torch.nn.Softmax(-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "            x = self.activation(x)\n",
    "        \n",
    "        x = self.prediction_head(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class DiscreteFailureTimeNLL(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, bin_boundaries, tolerance=1e-8):\n",
    "        super(DiscreteFailureTimeNLL, self).__init__()\n",
    "        \n",
    "        # Register as buffers so they move with the module\n",
    "        self.register_buffer('bin_starts', torch.tensor(bin_boundaries[:-1], dtype=torch.float32))\n",
    "        self.register_buffer('bin_ends', torch.tensor(bin_boundaries[1:], dtype=torch.float32))\n",
    "        self.register_buffer('bin_lengths', self.bin_ends - self.bin_starts)\n",
    "        \n",
    "        self.tolerance = tolerance\n",
    "    \n",
    "    def _discretize_times(self, times):\n",
    "        return (\n",
    "            (times[:, None] > self.bin_starts[None, :])\n",
    "            & (times[:, None] <= self.bin_ends[None, :])\n",
    "        )\n",
    "    \n",
    "    def _get_proportion_of_bins_completed(self, times):\n",
    "        return torch.clamp(\n",
    "            (times[:, None] - self.bin_starts[None, :]) / self.bin_lengths[None, :],\n",
    "            min=0.0,\n",
    "            max=1.0\n",
    "        )\n",
    "    \n",
    "    def forward(self, predictions, event_indicators, event_times):\n",
    "        # Ensure input tensors are on the same device as the model\n",
    "        event_indicators = event_indicators.to(predictions.device)\n",
    "        event_times = event_times.to(predictions.device)\n",
    "        \n",
    "        event_likelihood = torch.sum(\n",
    "            self._discretize_times(event_times) * predictions[:, :-1],\n",
    "            dim=-1\n",
    "        ) + self.tolerance\n",
    "        \n",
    "        nonevent_likelihood = 1 - torch.sum(\n",
    "            self._get_proportion_of_bins_completed(event_times) * predictions[:, :-1],\n",
    "            dim=-1\n",
    "        ) + self.tolerance\n",
    "        \n",
    "        log_likelihood = event_indicators * torch.log(event_likelihood)\n",
    "        log_likelihood += (1 - event_indicators) * torch.log(nonevent_likelihood)\n",
    "        \n",
    "        return -1. * torch.mean(log_likelihood)\n",
    "\n",
    "def preprocess_data(data, \n",
    "                   numerical_cols=['age', 'eGFR', 'sbp', 'bmi', 'tc', 'hdlc'],\n",
    "                   categorical_cols=['diabetes', 'smoker', 'antihtn', 'statin'],\n",
    "                   scaler=None):\n",
    "    \"\"\"\n",
    "    Preprocess data: standardize numerical columns, ensure categorical are numeric\n",
    "    \"\"\"\n",
    "    processed_data = data.copy()\n",
    "    \n",
    "    # Handle numerical columns\n",
    "    if scaler is None:\n",
    "        # Training phase - fit scaler\n",
    "        scaler = StandardScaler()\n",
    "        numerical_data = processed_data[numerical_cols]\n",
    "        processed_data[numerical_cols] = scaler.fit_transform(numerical_data)\n",
    "    else:\n",
    "        # Test phase - apply existing scaler\n",
    "        numerical_data = processed_data[numerical_cols]\n",
    "        processed_data[numerical_cols] = scaler.transform(numerical_data)\n",
    "    \n",
    "    # Handle categorical columns - ensure they're numeric\n",
    "    for col in categorical_cols:\n",
    "        if col in processed_data.columns:\n",
    "            processed_data[col] = processed_data[col].astype(float)\n",
    "    \n",
    "    return {'data': processed_data, 'scaler': scaler}\n",
    "\n",
    "def get_batches(*arrs, batch_size=1):\n",
    "    \"\"\"Generate batches of data\"\"\"\n",
    "    l = len(arrs[0])\n",
    "    for ndx in range(0, l, batch_size):\n",
    "        yield [torch.tensor(arr[ndx:min(ndx + batch_size, l)], dtype=torch.float32) for arr in arrs]\n",
    "\n",
    "def create_time_bins(times, num_bins=10):\n",
    "    \"\"\"Create time bins for discretization\"\"\"\n",
    "    max_time = np.max(times)\n",
    "    bin_boundaries = np.linspace(0, max_time, num_bins + 1)\n",
    "    return bin_boundaries\n",
    "\n",
    "def perform_cv_discrete_nn_with_validation(data, \n",
    "                                         event_var, \n",
    "                                         time_var,\n",
    "                                         prediction_times=[5, 10],\n",
    "                                         folds=5,\n",
    "                                         hidden_layers=[64, 32],\n",
    "                                         num_bins=20,\n",
    "                                         epochs=200,  # Increased since we'll use early stopping\n",
    "                                         learning_rate=0.001,\n",
    "                                         batch_size=100,\n",
    "                                         stratify_var=\"sex\",\n",
    "                                         validation_split=0.1,  # 10% for validation\n",
    "                                         patience=15,           # Early stopping patience\n",
    "                                         min_delta=1e-4):       # Minimum improvement threshold\n",
    "    \"\"\"\n",
    "    Enhanced CV with validation split and early stopping\n",
    "    \n",
    "    Args:\n",
    "        validation_split: Fraction of training data to use for validation\n",
    "        patience: Number of epochs to wait for improvement before stopping\n",
    "        min_delta: Minimum change to qualify as an improvement\n",
    "    \"\"\"\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(123)\n",
    "    np.random.seed(123)\n",
    "    \n",
    "    # Initialize result arrays\n",
    "    results = {}\n",
    "    for pred_time in prediction_times:\n",
    "        results[f\"year_{pred_time}\"] = np.zeros(len(data))\n",
    "    \n",
    "    # Define feature columns\n",
    "    feature_cols = ['age', 'eGFR', 'sbp', 'bmi', 'tc', 'hdlc', \n",
    "                   'diabetes', 'smoker', 'antihtn', 'statin']\n",
    "    \n",
    "    # Perform stratified CV by sex\n",
    "    if stratify_var and stratify_var in data.columns:\n",
    "        sex_groups = data[stratify_var].unique()\n",
    "        \n",
    "        for sex_val in sex_groups:\n",
    "            print(f\"Processing sex group: {sex_val} ({'Female' if sex_val == 1 else 'Male'})\")\n",
    "            \n",
    "            # Subset data for current sex group\n",
    "            sex_mask = data[stratify_var] == sex_val\n",
    "            sex_data = data[sex_mask].copy()\n",
    "            sex_indices = np.where(sex_mask)[0]\n",
    "            \n",
    "            # Create stratified folds within this sex group\n",
    "            skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=123)\n",
    "            \n",
    "            for fold_idx, (train_idx, test_idx) in enumerate(skf.split(sex_data, sex_data[event_var])):\n",
    "                print(f\"  Processing sex {sex_val} fold {fold_idx + 1} of {folds}\")\n",
    "                \n",
    "                # Get train/test data\n",
    "                train_data = sex_data.iloc[train_idx].copy()\n",
    "                test_data = sex_data.iloc[test_idx].copy()\n",
    "                \n",
    "                # Split training data into train/validation\n",
    "                if validation_split > 0:\n",
    "                    train_train_idx, train_val_idx = train_test_split(\n",
    "                        range(len(train_data)),\n",
    "                        test_size=validation_split,\n",
    "                        stratify=train_data[event_var],\n",
    "                        random_state=123 + fold_idx\n",
    "                    )\n",
    "                    \n",
    "                    val_data = train_data.iloc[train_val_idx].copy()\n",
    "                    train_data = train_data.iloc[train_train_idx].copy()\n",
    "                    \n",
    "                    print(f\"    Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
    "                \n",
    "                # Convert local test indices to global indices\n",
    "                test_index_global = sex_indices[test_idx]\n",
    "                \n",
    "                # Preprocess data\n",
    "                train_processed = preprocess_data(train_data, scaler=None)\n",
    "                train_features = train_processed['data'][feature_cols]\n",
    "                scaler = train_processed['scaler']\n",
    "                \n",
    "                # Process validation data\n",
    "                if validation_split > 0:\n",
    "                    val_processed = preprocess_data(val_data, scaler=scaler)\n",
    "                    val_features = val_processed['data'][feature_cols]\n",
    "                    val_features_np = val_features.values\n",
    "                    val_events_np = val_data[event_var].values\n",
    "                    val_times_np = val_data[time_var].values\n",
    "                \n",
    "                # Process test data\n",
    "                test_processed = preprocess_data(test_data, scaler=scaler)\n",
    "                test_features = test_processed['data'][feature_cols]\n",
    "                \n",
    "                # Create time bins based on training data\n",
    "                bin_boundaries = create_time_bins(train_data[time_var].values, num_bins)\n",
    "                \n",
    "                # Convert to numpy arrays\n",
    "                train_features_np = train_features.values\n",
    "                test_features_np = test_features.values\n",
    "                train_events_np = train_data[event_var].values\n",
    "                train_times_np = train_data[time_var].values\n",
    "                \n",
    "                # Initialize model and move to device\n",
    "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                model = DiscreteTimeNN(hidden_layers, num_bins).to(device)\n",
    "                loss_fn = DiscreteFailureTimeNLL(bin_boundaries).to(device)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                \n",
    "                # Early stopping variables\n",
    "                best_val_loss = float('inf')\n",
    "                best_model_state = None\n",
    "                epochs_without_improvement = 0\n",
    "                \n",
    "                # Training loop with validation\n",
    "                train_losses = []\n",
    "                val_losses = []\n",
    "                \n",
    "                for epoch in range(epochs):\n",
    "                    # Training phase\n",
    "                    model.train()\n",
    "                    epoch_train_losses = []\n",
    "                    \n",
    "                    for batch_data in get_batches(train_features_np, train_events_np, train_times_np, batch_size=batch_size):\n",
    "                        batch_X, batch_s, batch_t = batch_data\n",
    "                        batch_X = batch_X.to(device)\n",
    "                        batch_s = batch_s.to(device)\n",
    "                        batch_t = batch_t.to(device)\n",
    "                        \n",
    "                        optimizer.zero_grad()\n",
    "                        predictions = model(batch_X)\n",
    "                        loss = loss_fn(predictions, batch_s, batch_t)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        epoch_train_losses.append(loss.item())\n",
    "                    \n",
    "                    avg_train_loss = np.mean(epoch_train_losses)\n",
    "                    train_losses.append(avg_train_loss)\n",
    "                    \n",
    "                    # Validation phase\n",
    "                    if validation_split > 0:\n",
    "                        model.eval()\n",
    "                        val_epoch_losses = []\n",
    "                        \n",
    "                        with torch.no_grad():\n",
    "                            for batch_data in get_batches(val_features_np, val_events_np, val_times_np, batch_size=batch_size):\n",
    "                                batch_X, batch_s, batch_t = batch_data\n",
    "                                batch_X = batch_X.to(device)\n",
    "                                batch_s = batch_s.to(device)\n",
    "                                batch_t = batch_t.to(device)\n",
    "                                \n",
    "                                predictions = model(batch_X)\n",
    "                                loss = loss_fn(predictions, batch_s, batch_t)\n",
    "                                val_epoch_losses.append(loss.item())\n",
    "                        \n",
    "                        avg_val_loss = np.mean(val_epoch_losses)\n",
    "                        val_losses.append(avg_val_loss)\n",
    "                        \n",
    "                        # Early stopping logic\n",
    "                        if avg_val_loss < best_val_loss - min_delta:\n",
    "                            best_val_loss = avg_val_loss\n",
    "                            best_model_state = model.state_dict().copy()\n",
    "                            epochs_without_improvement = 0\n",
    "                        else:\n",
    "                            epochs_without_improvement += 1\n",
    "                        \n",
    "                        # Print progress\n",
    "                        if (epoch + 1) % 20 == 0:\n",
    "                            print(f\"    Sex {sex_val} Fold {fold_idx + 1} Epoch {epoch + 1}\")\n",
    "                            print(f\"      Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "                            print(f\"      Best Val Loss: {best_val_loss:.4f}, Patience: {epochs_without_improvement}/{patience}\")\n",
    "                        \n",
    "                        # Early stopping\n",
    "                        if epochs_without_improvement >= patience:\n",
    "                            print(f\"    Early stopping at epoch {epoch + 1}\")\n",
    "                            break\n",
    "                    else:\n",
    "                        # No validation - just print training loss\n",
    "                        if (epoch + 1) % 20 == 0:\n",
    "                            print(f\"    Sex {sex_val} Fold {fold_idx + 1} Epoch {epoch + 1} Train Loss: {avg_train_loss:.4f}\")\n",
    "                \n",
    "                # Load best model if using validation\n",
    "                if validation_split > 0 and best_model_state is not None:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                    print(f\"    Loaded best model with validation loss: {best_val_loss:.4f}\")\n",
    "                \n",
    "                # Make predictions on test set\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    test_x = torch.tensor(test_features_np, dtype=torch.float32).to(device)\n",
    "                    test_predictions = model(test_x)\n",
    "                    \n",
    "                    # Calculate survival probabilities for each prediction time\n",
    "                    for pred_time in prediction_times:\n",
    "                        bin_idx = np.where(bin_boundaries[:-1] <= pred_time)[0]\n",
    "                        if len(bin_idx) > 0:\n",
    "                            cum_event_prob = torch.sum(test_predictions[:, bin_idx], dim=-1)\n",
    "                            event_prob = torch.clamp(cum_event_prob, 0, 1)\n",
    "                            results[f\"year_{pred_time}\"][test_index_global] = event_prob.cpu().numpy()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def predict_multiple_events_enhanced(data,\n",
    "                                   events=['cvd', 'ascvd', 'hf'],\n",
    "                                   prediction_times=[5, 10],\n",
    "                                   folds=5,\n",
    "                                   hidden_layers=[64, 32],\n",
    "                                   num_bins=20,\n",
    "                                   epochs=200,\n",
    "                                   learning_rate=0.001,\n",
    "                                   batch_size=100,\n",
    "                                   validation_split=0.1,\n",
    "                                   patience=15):\n",
    "    \"\"\"\n",
    "    Enhanced version with validation and early stopping\n",
    "    \"\"\"\n",
    "    result_data = data.copy()\n",
    "    \n",
    "    for event in events:\n",
    "        for h in prediction_times:\n",
    "            event_var = f\"{event}_{h}y\"\n",
    "            time_var = f\"time2{event}_{h}y\"\n",
    "            \n",
    "            print(f\"CV for {event_var} using {time_var}\")\n",
    "            \n",
    "            if event_var not in data.columns or time_var not in data.columns:\n",
    "                print(f\"Warning: Required columns not found\")\n",
    "                continue\n",
    "            \n",
    "            # Run enhanced CV\n",
    "            cv_results = perform_cv_discrete_nn_with_validation(\n",
    "                data=data,\n",
    "                event_var=event_var,\n",
    "                time_var=time_var,\n",
    "                prediction_times=[h],\n",
    "                folds=folds,\n",
    "                hidden_layers=hidden_layers,\n",
    "                num_bins=num_bins,\n",
    "                epochs=epochs,\n",
    "                learning_rate=learning_rate,\n",
    "                batch_size=batch_size,\n",
    "                validation_split=validation_split,\n",
    "                patience=patience\n",
    "            )\n",
    "            \n",
    "            out_col = f\"{h}y_dnn_{event}\"\n",
    "            result_data[out_col] = cv_results[f\"year_{h}\"]\n",
    "    \n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55214edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predict_multiple_events_enhanced(\n",
    "    data=data,\n",
    "    events=['cvd', 'ascvd', 'hf'],\n",
    "    prediction_times=[5, 10],\n",
    "    folds=5,\n",
    "    hidden_layers=[64, 32],\n",
    "    num_bins=20,\n",
    "    epochs=200,\n",
    "    learning_rate=0.001,\n",
    "    validation_split=0.1,\n",
    "    patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c3791",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de502a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('../../data/strict_cohort2/all_data.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
