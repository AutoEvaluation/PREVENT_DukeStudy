{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f32d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5942d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class DiscreteTimeNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes, num_bins):\n",
    "        super(DiscreteTimeNN, self).__init__()\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            torch.nn.LazyLinear(size)\n",
    "            for size in hidden_layer_sizes\n",
    "        ])\n",
    "        \n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.prediction_head = torch.nn.LazyLinear(num_bins + 1)\n",
    "        self.softmax = torch.nn.Softmax(-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "            x = self.activation(x)\n",
    "        \n",
    "        x = self.prediction_head(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class DiscreteFailureTimeNLL(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, bin_boundaries, tolerance=1e-8):\n",
    "        super(DiscreteFailureTimeNLL, self).__init__()\n",
    "        \n",
    "        # Register as buffers so they move with the module\n",
    "        self.register_buffer('bin_starts', torch.tensor(bin_boundaries[:-1], dtype=torch.float32))\n",
    "        self.register_buffer('bin_ends', torch.tensor(bin_boundaries[1:], dtype=torch.float32))\n",
    "        self.register_buffer('bin_lengths', self.bin_ends - self.bin_starts)\n",
    "        \n",
    "        self.tolerance = tolerance\n",
    "    \n",
    "    def _discretize_times(self, times):\n",
    "        return (\n",
    "            (times[:, None] > self.bin_starts[None, :])\n",
    "            & (times[:, None] <= self.bin_ends[None, :])\n",
    "        )\n",
    "    \n",
    "    def _get_proportion_of_bins_completed(self, times):\n",
    "        return torch.clamp(\n",
    "            (times[:, None] - self.bin_starts[None, :]) / self.bin_lengths[None, :],\n",
    "            min=0.0,\n",
    "            max=1.0\n",
    "        )\n",
    "    \n",
    "    def forward(self, predictions, event_indicators, event_times):\n",
    "        # Ensure input tensors are on the same device as the model\n",
    "        event_indicators = event_indicators.to(predictions.device)\n",
    "        event_times = event_times.to(predictions.device)\n",
    "        \n",
    "        event_likelihood = torch.sum(\n",
    "            self._discretize_times(event_times) * predictions[:, :-1],\n",
    "            dim=-1\n",
    "        ) + self.tolerance\n",
    "        \n",
    "        nonevent_likelihood = 1 - torch.sum(\n",
    "            self._get_proportion_of_bins_completed(event_times) * predictions[:, :-1],\n",
    "            dim=-1\n",
    "        ) + self.tolerance\n",
    "        \n",
    "        log_likelihood = event_indicators * torch.log(event_likelihood)\n",
    "        log_likelihood += (1 - event_indicators) * torch.log(nonevent_likelihood)\n",
    "        \n",
    "        return -1. * torch.mean(log_likelihood)\n",
    "\n",
    "def preprocess_data(data, \n",
    "                   numerical_cols=['age', 'eGFR', 'sbp', 'bmi', 'tc', 'hdlc'],\n",
    "                   categorical_cols=['diabetes', 'smoker', 'antihtn', 'statin'],\n",
    "                   scaler=None):\n",
    "    \"\"\"\n",
    "    Preprocess data: standardize numerical columns, ensure categorical are numeric\n",
    "    \"\"\"\n",
    "    processed_data = data.copy()\n",
    "    \n",
    "    # Handle numerical columns\n",
    "    if scaler is None:\n",
    "        # Training phase - fit scaler\n",
    "        scaler = StandardScaler()\n",
    "        numerical_data = processed_data[numerical_cols]\n",
    "        processed_data[numerical_cols] = scaler.fit_transform(numerical_data)\n",
    "    else:\n",
    "        # Test phase - apply existing scaler\n",
    "        numerical_data = processed_data[numerical_cols]\n",
    "        processed_data[numerical_cols] = scaler.transform(numerical_data)\n",
    "    \n",
    "    # Handle categorical columns - ensure they're numeric\n",
    "    for col in categorical_cols:\n",
    "        if col in processed_data.columns:\n",
    "            processed_data[col] = processed_data[col].astype(float)\n",
    "    \n",
    "    return {'data': processed_data, 'scaler': scaler}\n",
    "\n",
    "def get_batches(*arrs, batch_size=1):\n",
    "    \"\"\"Generate batches of data\"\"\"\n",
    "    l = len(arrs[0])\n",
    "    for ndx in range(0, l, batch_size):\n",
    "        yield [torch.tensor(arr[ndx:min(ndx + batch_size, l)], dtype=torch.float32) for arr in arrs]\n",
    "\n",
    "def create_time_bins(times, num_bins=10):\n",
    "    \"\"\"Create time bins for discretization\"\"\"\n",
    "    max_time = np.max(times)\n",
    "    bin_boundaries = np.linspace(0, max_time, num_bins + 1)\n",
    "    return bin_boundaries\n",
    "\n",
    "def train_dnn_model(data, \n",
    "                   event_var, \n",
    "                   time_var,\n",
    "                   prediction_times=[5, 10],\n",
    "                   gender_val=None,\n",
    "                   hidden_layers=[64, 32],\n",
    "                   num_bins=20,\n",
    "                   epochs=200,\n",
    "                   learning_rate=0.001,\n",
    "                   batch_size=100,\n",
    "                   validation_split=0.1,\n",
    "                   patience=15,\n",
    "                   min_delta=1e-4):\n",
    "    \"\"\"\n",
    "    Train DNN model with validation split and early stopping\n",
    "    \"\"\"\n",
    "    # Filter by gender if specified\n",
    "    if gender_val is not None:\n",
    "        data_filtered = data[data['sex'] == gender_val].copy()\n",
    "    else:\n",
    "        data_filtered = data.copy()\n",
    "    \n",
    "    # Define feature columns\n",
    "    feature_cols = ['age', 'eGFR', 'sbp', 'bmi', 'tc', 'hdlc', \n",
    "                   'diabetes', 'smoker', 'antihtn', 'statin']\n",
    "    \n",
    "    # Split into train/validation\n",
    "    train_data, val_data = train_test_split(\n",
    "        data_filtered, \n",
    "        test_size=validation_split, \n",
    "        stratify=data_filtered[event_var], \n",
    "        random_state=123\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {len(train_data)}, Validation: {len(val_data)}\")\n",
    "    \n",
    "    # Preprocess data\n",
    "    train_processed = preprocess_data(train_data, scaler=None)\n",
    "    train_features = train_processed['data'][feature_cols]\n",
    "    scaler = train_processed['scaler']\n",
    "    \n",
    "    val_processed = preprocess_data(val_data, scaler=scaler)\n",
    "    val_features = val_processed['data'][feature_cols]\n",
    "    \n",
    "    # Create time bins based on training data\n",
    "    bin_boundaries = create_time_bins(train_data[time_var].values, num_bins)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    train_features_np = train_features.values\n",
    "    val_features_np = val_features.values\n",
    "    train_events_np = train_data[event_var].values\n",
    "    train_times_np = train_data[time_var].values\n",
    "    val_events_np = val_data[event_var].values\n",
    "    val_times_np = val_data[time_var].values\n",
    "    \n",
    "    # Initialize model and move to device\n",
    "    model = DiscreteTimeNN(hidden_layers, num_bins).to(device)\n",
    "    loss_fn = DiscreteFailureTimeNLL(bin_boundaries).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    # Training loop with validation\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        \n",
    "        for batch_data in get_batches(train_features_np, train_events_np, train_times_np, batch_size=batch_size):\n",
    "            batch_X, batch_s, batch_t = batch_data\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_s = batch_s.to(device)\n",
    "            batch_t = batch_t.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_X)\n",
    "            loss = loss_fn(predictions, batch_s, batch_t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_losses.append(loss.item())\n",
    "        \n",
    "        avg_train_loss = np.mean(epoch_train_losses)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_epoch_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data in get_batches(val_features_np, val_events_np, val_times_np, batch_size=batch_size):\n",
    "                batch_X, batch_s, batch_t = batch_data\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_s = batch_s.to(device)\n",
    "                batch_t = batch_t.to(device)\n",
    "                \n",
    "                predictions = model(batch_X)\n",
    "                loss = loss_fn(predictions, batch_s, batch_t)\n",
    "                val_epoch_losses.append(loss.item())\n",
    "        \n",
    "        avg_val_loss = np.mean(val_epoch_losses)\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if avg_val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch + 1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "            print(f\"Best Val Loss: {best_val_loss:.4f}, Patience: {epochs_without_improvement}/{patience}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"Loaded best model with validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'bin_boundaries': bin_boundaries,\n",
    "        'feature_cols': feature_cols\n",
    "    }\n",
    "\n",
    "def predict_with_dnn(model_dict, test_data, prediction_times=[5, 10]):\n",
    "    \"\"\"\n",
    "    Make predictions using trained DNN model\n",
    "    \"\"\"\n",
    "    model = model_dict['model']\n",
    "    scaler = model_dict['scaler']\n",
    "    bin_boundaries = model_dict['bin_boundaries']\n",
    "    feature_cols = model_dict['feature_cols']\n",
    "    \n",
    "    # Preprocess test data\n",
    "    test_processed = preprocess_data(test_data, scaler=scaler)\n",
    "    test_features = test_processed['data'][feature_cols]\n",
    "    test_features_np = test_features.values\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    predictions = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_x = torch.tensor(test_features_np, dtype=torch.float32).to(device)\n",
    "        test_predictions = model(test_x)\n",
    "        \n",
    "        # Calculate survival probabilities for each prediction time\n",
    "        for pred_time in prediction_times:\n",
    "            bin_idx = np.where(bin_boundaries[:-1] <= pred_time)[0]\n",
    "            if len(bin_idx) > 0:\n",
    "                cum_event_prob = torch.sum(test_predictions[:, bin_idx], dim=-1)\n",
    "                event_prob = torch.clamp(cum_event_prob, 0, 1)\n",
    "                predictions[f\"{pred_time}y\"] = event_prob.cpu().numpy()\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466540d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "strict = pd.read_csv('../../data/strict_cohort2/all_data.csv')\n",
    "data = pd.read_csv('../../data/relax_cohort2/all_data.csv')  # Assuming this is your main data\n",
    "    \n",
    "    # Initialize new columns\n",
    "new_cols = [\"10y_dnn_cvd\", \"10y_dnn_ascvd\", \"10y_dnn_hf\", \n",
    "            \"5y_dnn_cvd\", \"5y_dnn_ascvd\", \"5y_dnn_hf\"]\n",
    "for col in new_cols:\n",
    "    data[col] = np.nan\n",
    "    \n",
    "    print(f\"Initialized columns: {new_cols}\")\n",
    "    print(f\"Data columns containing 'dnn': {[col for col in data.columns if 'dnn' in col]}\")\n",
    "    \n",
    "    # Stage 1: Copy paste existing predictions for overlapping patients\n",
    "    print(\"Stage 1: Copying existing predictions...\")\n",
    "    print(f\"Strict data columns containing 'dnn': {[col for col in strict.columns if 'dnn' in col]}\")\n",
    "    \n",
    "    # Set PATID as index for both dataframes for easier matching\n",
    "    data_indexed = data.set_index('PATID')\n",
    "    strict_indexed = strict.set_index('PATID')\n",
    "    \n",
    "    # Copy existing predictions directly\n",
    "    for col in new_cols:\n",
    "        if col in strict_indexed.columns:\n",
    "            # Find overlapping patients\n",
    "            overlapping_patients = data_indexed.index.intersection(strict_indexed.index)\n",
    "            # Copy non-null values from strict to relax data\n",
    "            strict_values = strict_indexed.loc[overlapping_patients, col]\n",
    "            non_null_mask = strict_values.notna()\n",
    "            if non_null_mask.sum() > 0:\n",
    "                data_indexed.loc[overlapping_patients[non_null_mask], col] = strict_values[non_null_mask]\n",
    "                print(f\"Copied {non_null_mask.sum()} values for {col}\")\n",
    "    \n",
    "    # Reset index back to regular dataframe\n",
    "    data = data_indexed.reset_index()\n",
    "    \n",
    "    # Stage 2: Train DNN models on all strict data and predict for missing values\n",
    "    print(\"Stage 2: Training DNN models...\")\n",
    "    \n",
    "    # Define model specifications\n",
    "    model_specs = [\n",
    "        {'event': 'cvd', 'prediction_times': [5, 10]},\n",
    "        {'event': 'ascvd', 'prediction_times': [5, 10]},\n",
    "        {'event': 'hf', 'prediction_times': [5, 10]}\n",
    "    ]\n",
    "    \n",
    "    # Create model directory if it doesn't exist\n",
    "    os.makedirs('../model', exist_ok=True)\n",
    "    \n",
    "    for spec in model_specs:\n",
    "        event = spec['event']\n",
    "        prediction_times = spec['prediction_times']\n",
    "        \n",
    "        for pred_time in prediction_times:\n",
    "            event_var = f\"{event}_{pred_time}y\"\n",
    "            time_var = f\"time2{event}_{pred_time}y\"\n",
    "            \n",
    "            if event_var not in strict.columns or time_var not in strict.columns:\n",
    "                print(f\"Warning: Required columns {event_var} or {time_var} not found in strict data\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Training models for {event} {pred_time}y...\")\n",
    "            \n",
    "            # Train separate models for each gender\n",
    "            for gender_val in [0, 1]:\n",
    "                gender_str = \"female\" if gender_val == 1 else \"male\"\n",
    "                print(f\"  Training {gender_str} model...\")\n",
    "                \n",
    "                # Train model\n",
    "                model_dict = train_dnn_model(\n",
    "                    data=strict,\n",
    "                    event_var=event_var,\n",
    "                    time_var=time_var,\n",
    "                    prediction_times=[pred_time],\n",
    "                    gender_val=gender_val,\n",
    "                    hidden_layers=[64, 32],\n",
    "                    num_bins=20,\n",
    "                    epochs=200,\n",
    "                    learning_rate=0.001,\n",
    "                    batch_size=100,\n",
    "                    validation_split=0.1,\n",
    "                    patience=15\n",
    "                )\n",
    "                \n",
    "                # Save model\n",
    "                model_filename = f\"../model/dnn_{event}_{pred_time}y_{gender_str}.pkl\"\n",
    "                with open(model_filename, 'wb') as f:\n",
    "                    pickle.dump(model_dict, f)\n",
    "                \n",
    "                print(f\"  Saved model to {model_filename}\")\n",
    "                \n",
    "                # Predict for missing values in main data\n",
    "                target_col = f\"{pred_time}y_dnn_{event}\"\n",
    "                \n",
    "                # Check if column exists, if not skip\n",
    "                if target_col not in data.columns:\n",
    "                    print(f\"  Warning: Column {target_col} not found in data, skipping...\")\n",
    "                    continue\n",
    "                \n",
    "                # Find rows where prediction is missing for this gender\n",
    "                gender_mask = data['sex'] == gender_val\n",
    "                missing_mask = data[target_col].isna()\n",
    "                target_mask = gender_mask & missing_mask\n",
    "                \n",
    "                if target_mask.sum() > 0:\n",
    "                    print(f\"  Predicting for {target_mask.sum()} missing {gender_str} patients...\")\n",
    "                    \n",
    "                    # Make predictions\n",
    "                    test_data = data[target_mask].copy()\n",
    "                    predictions = predict_with_dnn(model_dict, test_data, [pred_time])\n",
    "                    \n",
    "                    # Update main data\n",
    "                    data.loc[target_mask, target_col] = predictions[f\"{pred_time}y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f2a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528bc825",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../../data/relax_cohort2/all_data2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
